---
title: "DATA 607 - Project3 [Job Posts Web Data Analysis]"
author: "Simon U. & Ritesh Lohiya"
date: "March 25, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Job Postings Data Analysis {.tabset .tabset-fade}

## Data Analysis

####Libraries
```{r warning=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(readxl)
library(ggplot2)
```

####Combine NY and CA Jobs Data
```{r warning=FALSE}
data_store_path <- "~/R/Project3_Group3"

#load NY data into jobs.ny data frame
load(file.path(data_store_path, "jobs_df.RData"))
jobs.ny <- job_df
rm(job_df)
names(jobs.ny)[1] <- "job_post_region"
jobs.ny$job_post_region <- 'NY'
jobs.ny <- jobs.ny[,-which(names(jobs.ny) == "job_post_title")]
kable_styling(knitr::kable(head(jobs.ny, 3), "html", caption = "NY Data Frame"), bootstrap_options = "striped")


#load CA (San Francisco) data into jobs.ca data frame
load(file.path(data_store_path, "SanFrancisco_CA_searchAllJobUrls_and_job_sum_text_objects_after_remove_empty_and_duplicate_postings.Rdata"))
jobs.ca <- data.frame(job_post_region = "CA", job_post_summary = job_sum_text)
rm(job_sum_text, job_post_region, job_post_summary)
kable_styling(knitr::kable(head(jobs.ca, 3), "html", caption = "CA Data Frame"), bootstrap_options = "striped")

#Combine NY and CA data frames into jobs.df
jobs.df <- bind_rows(jobs.ny, jobs.ca)
rm(jobs.ny, jobs.ca)
```

####Prepare list of key terms - from Heather G. and Raj K.
The list of data science skills is based off the list found here: https://www.thebalance.com/list-of-data-scientist-skills-2062381
```{r warning=FALSE}
keywords <- read.table("https://raw.githubusercontent.com/heathergeiger/Data607_Project3_Group3/master/heathergeiger_individual_work/combine_ny_and_san_francisco/keywords.txt",header=TRUE,check.names=FALSE,stringsAsFactors=FALSE,sep="\t")
keywords <- keywords[grep('This is probably too tough',keywords$Other.notes,invert=TRUE),]
kable_styling(knitr::kable(head(keywords[,-4], 10), "html", caption = "Keywords"), bootstrap_options = "striped")

keyword_list <- vector("list",length=nrow(keywords))
for(i in 1:nrow(keywords)) {
  keywords_this_row <- keywords$Skill[i]
  if(keywords$Synonyms[i] != "None"){
    keywords_this_row <- c(keywords_this_row,unlist(strsplit(keywords$Synonyms[i],",")[[1]]))
  }
  keyword_list[[i]] <- keywords_this_row
}

space_or_comma <- "[[:space:],]"
word_boundary <- "\\b"
pattern_for_one_keyword <- function(keyword){
  regexes <- paste0(space_or_comma,keyword,space_or_comma)
  regexes <- c(regexes,paste0(word_boundary,keyword,word_boundary))
  regexes <- c(regexes,paste0(word_boundary,keyword,space_or_comma))
  regexes <- c(regexes,paste0(space_or_comma,keyword,word_boundary))
  return(paste0(regexes,collapse="|"))
}
pattern_for_multiple_keywords <- function(keyword_vector){
  if(length(keyword_vector) == 1){return(pattern_for_one_keyword(keyword_vector))}
  if(length(keyword_vector) > 1){
    individual_regexes <- c()
    for(i in 1:length(keyword_vector))
    {
      individual_regexes <- c(individual_regexes,pattern_for_one_keyword(keyword_vector[i]))
    }
    return(paste0(individual_regexes,collapse="|")) 
  }
}
keyword_regexes <- unlist(lapply(keyword_list,function(x)pattern_for_multiple_keywords(x)))
kable_styling(knitr::kable(head(keyword_regexes), "html", caption = "Regex of Keywords"), bootstrap_options = "striped")
```

####Compare keywords against jobs post summary data
```{r}
for(i in 1:length(keyword_regexes)) {
  jobs.df[,keywords$Skill[i]] <- NA
  skill <- keyword_regexes[i]
  new.skill.col <- unlist(str_detect(tolower(jobs.df$job_post_summary),skill))
  jobs.df[,keywords$Skill[i]] <- new.skill.col
}
kable_styling(knitr::kable(head(jobs.df[15:18,]), "html", caption = "Job Skills [in Wide Format]"), bootstrap_options = "striped")

jobs.df.long <- jobs.df %>% gather("Skill", "Appears", 3:length(jobs.df)) %>% inner_join(keywords) %>% select(-c(Synonyms, Other.notes))
kable_styling(knitr::kable(head(jobs.df.long[15:18,]), "html", caption = "Job Skills [in Long Format]"), bootstrap_options = "striped")
save(jobs.df.long, file = file.path(data_store_path, "jobs_df_long.RData"), ascii = TRUE)
```

####Analysing Results
```{r}
#Filter out Job Skills which did not appear
job.skills <- jobs.df.long %>% 
  filter(Appears == TRUE)

table(job.skills$Soft.or.technical, job.skills$Appears) %>% 
  knitr::kable("html", caption = "Soft vs. Technical") %>% kable_styling(bootstrap_options = "striped")

table(job.skills$Soft.or.technical, job.skills$Appears) %>%
  as.data.frame %>%
  ggplot(aes(x = Var1, y = Freq, fill = Var1)) +
    geom_col() +
    labs(title = "Job Results by Skill Type", x = "Skill Type", y = "Number of Results") + 
    scale_fill_discrete(name = "Skill Type") +
    geom_text(aes(label = Freq, y = Freq + 12), size = 5, position = position_dodge(0.9), vjust = 0)

n.jobs <- nrow(jobs.df)
job.skills %>%
  group_by(Soft.or.technical, Skill) %>% 
  summarize(Skill.Percent = 100 * sum(Appears == TRUE)/n.jobs) %>%
  ggplot(aes(x = reorder(Skill, Skill.Percent), Skill.Percent, fill = Soft.or.technical)) + 
    geom_bar(stat = 'identity', position = 'dodge') +
    coord_flip() +
    labs(title = "Top Data Scientist Skills", x = "Skills", y = "Percentage of Jobs with Skill") + 
    scale_fill_discrete(name = "Skill Type")
```

####Conclusion
Given that our jobs criteria was based on top-paying, senior level Data Scientists, the top 10 skills are very representative and realistic to expect for this type of data.  
* At the senior job levels, it's no wonder that **Leadership** soft skill tops all other soft and technical skills, with 2 other soft skills, **Communication** and **Collaboration** following closely in importance.  
* On the technical side, it's no surprise that at a senior level, **Modeling** and **Machine Learning** are expected as top technical skills.  Then we have **Python** and **R** wrapping around **Statistics** and finally followed by **SQL** and **Big Data** completing the top 10 skill set.

## Data Scraping (NY - Indeed.com)

####Libraries
```{r lib, warning=FALSE, message=FALSE, eval=FALSE}
library(RCurl)
library(XML)
library(tidyverse)
library(rvest)
library(stringr)
library(ggplot2)
```

####Get listing of 16 HTML files for the Data Scientist [from Indeed.com] job posts
```{r eval=FALSE}
#NOTE: provide an existing path (in your environment) in order to store generated output files
data_store_path <- "~/GitHub/Project3"

jobURLs <- list.files(data_store_path, "indeed_job_post_.*.html")
head(jobURLs, 3)
```

####Visit each job posting HTML file and scrape job title and description for analysis
```{r eval=FALSE}
job_sum_text <- vector(mode = "character", length = length(jobURLs))
job_title <- vector(mode = "character", length = length(jobURLs))

for (i in 1:length(jobURLs)) {
  #Visit each HTML page
  htmFile <- file.path(data_store_path, jobURLs[i])
  h <- read_html(htmFile)

  #Get HTML nodes with CSS id "job_summary"
  jobSum <- html_nodes(h, "#job_summary")
  
  #Get textual content from the "job summary"" nodes
  job_sum_text[i] = html_text(jobSum)

  #Collect job title text
  #Search for HTML <b> nodes with CSS class "jobtitle"
  jobTitleNode <- html_nodes(h, "b.jobtitle")
  job_title[i] <- html_text(jobTitleNode)
}
```

####Create a data frame holding the result of scraping (job title, job summary, etc.) and save to a file
```{r eval=FALSE}
job_df <- data.frame(job_post_source = "INDEED", job_post_title = job_title, job_post_summary = job_sum_text)
glimpse(job_df)
save(job_df, file = file.path(data_store_path, "jobs_df.RData"), ascii = TRUE)
```

####To load the data frame object [named job_df] back into the environment call:  
```{r eval=FALSE}
load(file.path(data_store_path, "jobs_df.RData"))
head(job_df, 2)
View(job_df)
```

## Data Scraping (CA - Monster.com)

#### Set URLs.

Set URLs based on the URLs for an actual search result in my browser.
Did it this way because this way can search for job title of "data scientist" (not just keyword search). 
Can also search for a reasonable radius around the city.

```{r, echo=TRUE, eval=FALSE}
new_york_url <- "https://www.monster.com/jobs/search/New-York+New-York-City+Data-Scientist_125?where=New-York__2c-NY&rad=20-miles"

san_francisco_url <- "https://www.monster.com/jobs/search/California+San-Francisco+Data-Scientist_125?where=San-Francisco__2c-CA&rad=20-miles"
```

#### Load libraries.

```{r, echo=TRUE, eval=FALSE}
library(stringr)    #For string operations
library(rvest)      #For screen scrapper
library(tokenizers) #
library(tidyverse)  #For Tidyverse
library(RCurl)      #For File Operations
library(dplyr)      #For Manipulating the data frames
library(DT)         #For Data table package
library(curl)
```

#### Set city to New York or San Francisco, then make output directory and pg. 1 URL object.

```{r, echo=TRUE,eval=FALSE, echo=FALSE}
#city2search <- "NewYork"
#state2search <- "NY"
#data_store_path <- paste0("job_postings_",city2search,"_",state2search)

#dir.create(data_store_path)

#searchPage_url <- new_york_url
```

```{r, echo=FALSE,eval=FALSE}
city2search <- "SanFrancisco"
state2search <- "CA"
data_store_path <- paste0("job_postings_",city2search,"_",state2search)

dir.create(data_store_path)

searchPage_url <- san_francisco_url
```

Run this part only once to avoid getting banned.

If you run subsequent times, load in from Rdata file.

Base URL gives first 25 results, then run pasting "&page=2", "&page=3", etc. to get all results.

Let's take the first 500 results per city, so the first 20 pages.

I checked and both New York and San Francisco have over 500 jobs in the search results.

```{r, echo=TRUE, eval=FALSE}
searchPage <- read_html(searchPage_url)

searchAllJobUrls <- unlist(str_extract_all(searchPage,'(job-openings\\.monster\\.com\\/)\\w.[^\\"]+'))
searchAllJobUrls <- paste("https://",searchAllJobUrls,sep = "")

searchAllJobUrls <- searchAllJobUrls[1:25]

for(page in 2:20)
{
searchPage <- read_html(paste0(searchPage_url,"&page=",page))
searchAllJobUrls_this_page <- unlist(str_extract_all(searchPage,'(job-openings\\.monster\\.com\\/)\\w.[^\\"]+'))
searchAllJobUrls_this_page <- paste("https://",searchAllJobUrls_this_page,sep = "")
searchAllJobUrls <- c(searchAllJobUrls,searchAllJobUrls_this_page[1:25])
}

save(searchAllJobUrls,file=paste0(data_store_path,"/searchAllJobUrls.Rdata"))
length(unique(tolower(searchAllJobUrls)))
```

If rerunning this script after already scraping the search results, set above to eval=FALSE and the below to eval=TRUE.

```{r, echo=TRUE,eval=FALSE}
load(paste0(data_store_path,"/searchAllJobUrls.Rdata"))
```

To make sure everything looks correct, show URLs 1, 26, and 51 so we can compare to the links we get by looking in a browser at search pages 1, 2, and 3.

```{r, echo=TRUE, eval=FALSE}
searchAllJobUrls[c(1,26,51)]
```

So, these match what we see by looking in browser results.

However, initially we found somewhat concerningly that the number of unique URLs is less than 500.

Looking manually through a few pages, it appears sometimes the same job will be listed under two different headlines (eg a "Data Scientist" job at Open Systems Technologies was listed as "Data Scientist" on pg2 and "Machine Learning Data Scientist" on pg3).

I think it should be fine to just run unique on searchAllJobUrls, and then proceed as normal.

```{r, echo=TRUE, eval=FALSE}
searchAllJobUrls <- unique(searchAllJobUrls)
length(searchAllJobUrls)
```

Now, read from each URL in searchAllJobUrls and save text in job description.

```{r, echo=TRUE,eval=FALSE}
job_sum_text <- vector(mode = "character", length = length(searchAllJobUrls))

for(i in 1:length(searchAllJobUrls))
{
h <- read_html(searchAllJobUrls[i])
forecasthtml <- html_nodes(h,"#JobDescription")
#Adding a check to ensure that there actually is a node with "JobDescription", as one of the URLs did not have this node and it broke the for loop.
if(length(forecasthtml) == 1)
{
        job_sum_text[i] <- html_text(forecasthtml)
}
if(length(forecasthtml) != 1)
{
        job_sum_text[i] <- "" #Add an empty string to job_sum_text for now for these. May want to delete these later on.
}
}

save(job_sum_text,file=paste0(data_store_path,"/job_description_text.Rdata"))
```

If rerunning this script after already scraping the job pages, set above to eval=FALSE and below to eval=TRUE.

```{r, echo=TRUE, eval=FALSE}
load(paste0(data_store_path,"/job_description_text.Rdata"))
```

```{r, echo=TRUE,eval=FALSE}
length(job_sum_text)
class(job_sum_text)
job_sum_text[1:3]
length(unique(job_sum_text))
```

When running a similar script for Columbus, OH, we found at least one job without a valid JobDescription node, so we put an empty string in text field.

We also found an instance of the same job clearly listed under two different URLs.

Let's check if this happens here, and remove such instances if so.

Then, save Rdata again.

```{r, echo=TRUE, eval=FALSE}
searchAllJobUrls <- searchAllJobUrls[job_sum_text != "" & duplicated(job_sum_text) == FALSE]
job_sum_text <- job_sum_text[job_sum_text != "" & duplicated(job_sum_text) == FALSE]

length(searchAllJobUrls)
length(job_sum_text)

save(list=c("searchAllJobUrls","job_sum_text"),
file=paste0(data_store_path,"/searchAllJobUrls_and_job_sum_text_objects_after_remove_empty_and_duplicate_postings.Rdata"))
```
